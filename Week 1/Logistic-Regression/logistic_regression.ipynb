{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION\n",
    "In this assignment we try to find a relation between average weight of granules and total surface area to see if a material is viable as a catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature mapping\n",
    "Sometimes, we are given an inadequate number of features for which training the dataset becomes difficult.  \n",
    "Hence we create new features of by taking polynomial products of existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map(points):\n",
    "    points = np.array(points)\n",
    "    x, y = points[:, 0], points[:, 1]\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # degree 0\n",
    "    features.append(np.ones_like(x))      # 1\n",
    "    \n",
    "    # degree 1\n",
    "    features.append(x)                     # x\n",
    "    features.append(y)                     # y\n",
    "    \n",
    "    # degree 2\n",
    "    features.append(x**2)                  # x^2\n",
    "    features.append(x*y)                   # xy\n",
    "    features.append(y**2)                  # y^2\n",
    "    \n",
    "    # degree 3\n",
    "    features.append(x**3)                  # x^3\n",
    "    features.append(x**2 * y)              # x^2y\n",
    "    features.append(x * y**2)              # xy^2\n",
    "    features.append(y**3)                  # y^3\n",
    "    \n",
    "    # degree 4\n",
    "    features.append(x**4)                  # x^4\n",
    "    features.append(x**3 * y)              # x^3y\n",
    "    features.append(x**2 * y**2)            # x^2y^2\n",
    "    features.append(x * y**3)              # xy^3\n",
    "    features.append(y**4)                  # y^4\n",
    "    \n",
    "    return np.column_stack(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the class for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.weights: np.ndarray | None = None\n",
    "        self.bias: float | None = None\n",
    "\n",
    "\n",
    "    # Sigmoid function\n",
    "    ### TODO 2\n",
    "    def __sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "    # Returns probabilities of being true\n",
    "    ### TODO 3\n",
    "    def predict_probability(self, X: np.ndarray) -> np.ndarray:\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.__sigmoid(z)\n",
    "\n",
    "\n",
    "    # Returns true/false (based on the probabilities)\n",
    "    ### TODO 4\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        probs = self.predict_probability(X)\n",
    "        return (probs >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "    # Returns loss, gradient wrt weights (dw), gradient wrt bias (db)\n",
    "    ### TODO 5 \n",
    "    def __loss(self, X: np.ndarray, y: np.ndarray, lambda_reg: float = 0) -> tuple:\n",
    "        m = X.shape[0]\n",
    "\n",
    "        y_pred = self.predict_probability(X)\n",
    "\n",
    "        # Binary cross-entropy loss\n",
    "        loss = (-1 / m) * np.sum(\n",
    "            y * np.log(y_pred + 1e-9) + (1 - y) * np.log(1 - y_pred + 1e-9)\n",
    "        )\n",
    "\n",
    "        # Gradients\n",
    "        dw = (1 / m) * np.dot(X.T, (y_pred - y))\n",
    "        db = (1 / m) * np.sum(y_pred - y)\n",
    "\n",
    "        # L2 Regularization\n",
    "        loss += (lambda_reg / (2 * m)) * np.sum(self.weights ** 2)\n",
    "        dw += (lambda_reg / m) * self.weights\n",
    "\n",
    "        return loss, dw, db\n",
    "\n",
    "\n",
    "    # Training using gradient descent\n",
    "    ### TODO 6\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 500,\n",
    "            learning_rate: float = 0.01, threshold: float = 0.0001, \n",
    "            lambda_reg: float = 1) -> None:\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "\n",
    "        prev_loss = float('inf')\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            loss, dw, db = self.__loss(X, y, lambda_reg)\n",
    "\n",
    "            # Gradient descent update\n",
    "            self.weights -= learning_rate * dw\n",
    "            self.bias -= learning_rate * db\n",
    "\n",
    "            # Early stopping\n",
    "            if abs(prev_loss - loss) < threshold:\n",
    "                break\n",
    "\n",
    "            prev_loss = loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "df = pd.read_csv('logistic_data.csv')\n",
    "data = df.to_numpy()\n",
    "X = data[:, :2]\n",
    "y = data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(X: np.ndarray) -> tuple:\n",
    "    # Mean and standard deviation per feature (column-wise)\n",
    "    x_mean = np.mean(X, axis=0)\n",
    "    x_std = np.std(X, axis=0)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    x_std = np.where(x_std == 0, 1, x_std)\n",
    "\n",
    "    # Z-score normalization\n",
    "    x = (X - x_mean) / x_std\n",
    "\n",
    "    return x, x_mean, x_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data (we use the same constants to maintain consistency)\n",
    "X_train, x_mean, x_std = z_score(X_train)\n",
    "X_test = (X_test - x_mean) / x_std\n",
    "x_train = feature_map(X_train)\n",
    "x_test = feature_map(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing how the boundary curve looks like\n",
    "def plot_decision_boundary(X_original, y, model, resolution=500):\n",
    "    # Set up the grid for the decision boundary\n",
    "    x_min, x_max = X_original[:, 0].min() - 1, X_original[:, 0].max() + 1\n",
    "    y_min, y_max = X_original[:, 1].min() - 1, X_original[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution),\n",
    "                         np.linspace(y_min, y_max, resolution))\n",
    "    \n",
    "    # Flatten the grid points and map to expanded features\n",
    "    grid_original = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_expanded = feature_map(grid_original)\n",
    "    \n",
    "    # Predict the grid values for decision boundary\n",
    "    Z = model.predict(grid_expanded)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the data points\n",
    "    true_points = X_original[y == 1]\n",
    "    false_points = X_original[y == 0]\n",
    "    plt.scatter(true_points[:, 0], true_points[:, 1], label=\"True\", c=\"blue\", marker=\"o\", s=20)\n",
    "    plt.scatter(false_points[:, 0], false_points[:, 1], label=\"False\", c=\"red\", marker=\"x\", s=20)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors=\"black\", linewidths=2)\n",
    "    \n",
    "    # Labeling and title\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(\"Decision Boundary and Data Points\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the decision boundary that the model predicts. This can be used to check for overfitting.  \n",
    "If the boundary starts looking like an ameoba trying to fit every point, then it is a sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model has an accuracy of 87.5%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    learning_rate=0.05,\n",
    "    threshold=1e-5,\n",
    "    lambda_reg=0.1\n",
    ")\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test) * 100\n",
    "print(f\"Your model has an accuracy of {accuracy}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wids_repo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
